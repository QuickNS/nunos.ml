{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Underfitting** occurs when your model cannot obtain sufficiently low loss on the training set. In this case, your model fails to learn the underlying patterns in your training data. On the other end of the spectrum, we have **overfitting** where your network models the training data too well and fails to generalize to your validation data.\n",
    "\n",
    "Therefore, our goal when training a machine learning model is to:\n",
    "1.  Reduce the training loss as much as possible.\n",
    "2.  While ensuring the gap between the training and testing loss is reasonably small.\n",
    "\n",
    "To combat overfitting, in general there are two techniques:\n",
    "1. Reduce the complexity of the model\n",
    "2. Apply regularization - highly prefered solution\n",
    "\n",
    "**Sometimes, overfitting is an invitability. What we need is to control the 'generalization gap'**. At some point learning and validation loss will start diverging and we need to try and keep the gap under control. If we start seeing a rise in validation loss, we are stronly overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from ndl.callbacks import TrainingMonitor\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from ndl.nn.conv import MiniVGGNet\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO process ID: 14140\n"
     ]
    }
   ],
   "source": [
    "# show information on the process ID\n",
    "print(\"[INFO process ID: {}\".format(os.getpid()))\n",
    "output = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading CIFAR-10 data...\n"
     ]
    }
   ],
   "source": [
    "# load the training and testing data, then scale it into the\n",
    "# range [0, 1]\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\") / 255.0\n",
    "testX = testX.astype(\"float\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# initialize the SGD optimizer, but without any learning rate decay\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model = MiniVGGNet.build(width=32, height=32, depth=3, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not including any decay on purpose to demonstrate monitoring and spot overfitting as it's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the set of callbacks\n",
    "figPath = os.path.sep.join([output, \"{}.png\".format(os.getpid())])\n",
    "jsonPath = os.path.sep.join([output, \"{}.json\".format(os.getpid())])\n",
    "callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you start training, take a look at your output directory and you should see the training history and loss plots being stored.\n",
    "\n",
    "- At epoch 5 we should still be underfitting\n",
    "- At epoch 10 we see the first signs of overfitting, but perfectly normal\n",
    "- At epoch 25 we see validation loss stagnating while training loss continues to go down.\n",
    "- At epoch 50 we are clearly in trouble with validation loss going up. We should have stopped the experiment here and not waste more time. Re-assess parameters, try again...\n",
    "\n",
    "If we let this go all over to epoch 100 we see our validation loss continuing to rise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=64, epochs=100, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
